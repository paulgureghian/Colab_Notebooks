{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Medetec_Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulgureghian/Google_Colab_PyTorch_Notebooks/blob/master/Medetec_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fqQih7hWMmaK",
        "colab_type": "code",
        "outputId": "92689316-e50a-4bd9-ec5d-69025a1b4f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lwomz8yNOD5M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from collections import OrderedDict\n",
        "from torch.optim import lr_scheduler \n",
        "from torchvision import  models, datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpJdybPMOgfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/Medetec/data/medetec_dataset'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/valid'\n",
        "test_dir = data_dir + '/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RPI0h2et6UWk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_transforms= transforms.Compose([transforms.RandomRotation(45),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.RandomCrop(224),\n",
        "                                         transforms.RandomResizedCrop(224),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                              (0.229,0.224,0.255))])\n",
        "\n",
        "validation_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                            transforms.CenterCrop(224),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                                 (0.229,0.224,0.255))])\n",
        "\n",
        "testing_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                         transforms.CenterCrop(224),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                              (0.229,0.224,0.255))])\n",
        "\n",
        "training_dataset = datasets.ImageFolder(train_dir, transform=training_transforms)\n",
        "validation_dataset = datasets.ImageFolder(valid_dir, transform=validation_transforms)\n",
        "testing_dataset = datasets.ImageFolder(test_dir, transform=testing_transforms)\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=32)\n",
        "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=32) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "msdlit-I_Qm9",
        "colab_type": "code",
        "outputId": "9776debc-6a09-4fa9-db7a-b9b5dffcb53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = models.vgg19(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad_(False) \n",
        "    \n",
        "model.classifier = nn.Sequential(OrderedDict([\n",
        "                                ('fc1', nn.Linear(25088, 4096)),\n",
        "                                ('relu', nn.ReLU()),\n",
        "                                ('fc2', nn.Linear(4096, 15)),\n",
        "                                ('output', nn.Softmax(dim=1))\n",
        "                                ])) \n",
        "\n",
        "epochs = 50\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.0001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) \n",
        "\n",
        "model.to(device) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.torch/models/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 574673361/574673361 [00:15<00:00, 38095445.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (fc1): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (relu): ReLU()\n",
              "    (fc2): Linear(in_features=4096, out_features=15, bias=True)\n",
              "    (output): Softmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "_e2ZWlc7TKTC",
        "colab_type": "code",
        "outputId": "6fda4ab2-314a-45cf-fced-baf0a43e2b7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    scheduler.step()\n",
        "    train_loss = 0\n",
        "    \n",
        "    model.train()\n",
        "    for inputs, labels in training_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logps = model.forward(inputs)\n",
        "        \n",
        "        loss = criterion(logps, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    model.eval()\n",
        "    accuracy = 0\n",
        "    valid_loss = 0\n",
        "    \n",
        "    for inputs, labels in validation_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        output = model.forward(inputs)\n",
        "        \n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        valid_loss += loss.item()\n",
        "        \n",
        "        ps = torch.exp(output)\n",
        "        top_ps, top_class = ps.topk(1, dim=1) \n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "        \n",
        "    print('Epoch {}/{}.. '.format(epoch+1, epochs),\n",
        "          'Training loss: {:.3f}.. '.format(train_loss / len(training_dataloader)),\n",
        "          'Validation loss: {:.3f}.. '.format(valid_loss / len(validation_dataloader)),\n",
        "          'Validation accuracy: {:.3f}.. '.format(accuracy / len(validation_dataloader)))\n",
        "          \n",
        "    model.train()      "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50..  Training loss: 2.593..  Validation loss: 2.562..  Validation accuracy: 0.245.. \n",
            "Epoch 2/50..  Training loss: 2.550..  Validation loss: 2.562..  Validation accuracy: 0.245.. \n",
            "Epoch 3/50..  Training loss: 2.552..  Validation loss: 2.501..  Validation accuracy: 0.323.. \n",
            "Epoch 4/50..  Training loss: 2.555..  Validation loss: 2.533..  Validation accuracy: 0.284.. \n",
            "Epoch 5/50..  Training loss: 2.475..  Validation loss: 2.503..  Validation accuracy: 0.331.. \n",
            "Epoch 6/50..  Training loss: 2.538..  Validation loss: 2.538..  Validation accuracy: 0.276.. \n",
            "Epoch 7/50..  Training loss: 2.452..  Validation loss: 2.523..  Validation accuracy: 0.292.. \n",
            "Epoch 8/50..  Training loss: 2.461..  Validation loss: 2.515..  Validation accuracy: 0.308.. \n",
            "Epoch 9/50..  Training loss: 2.490..  Validation loss: 2.532..  Validation accuracy: 0.284.. \n",
            "Epoch 10/50..  Training loss: 2.526..  Validation loss: 2.514..  Validation accuracy: 0.295.. \n",
            "Epoch 11/50..  Training loss: 2.440..  Validation loss: 2.493..  Validation accuracy: 0.320.. \n",
            "Epoch 12/50..  Training loss: 2.545..  Validation loss: 2.535..  Validation accuracy: 0.280.. \n",
            "Epoch 13/50..  Training loss: 2.514..  Validation loss: 2.508..  Validation accuracy: 0.308.. \n",
            "Epoch 14/50..  Training loss: 2.505..  Validation loss: 2.532..  Validation accuracy: 0.284.. \n",
            "Epoch 15/50..  Training loss: 2.430..  Validation loss: 2.511..  Validation accuracy: 0.298.. \n",
            "Epoch 16/50..  Training loss: 2.483..  Validation loss: 2.490..  Validation accuracy: 0.319.. \n",
            "Epoch 17/50..  Training loss: 2.504..  Validation loss: 2.544..  Validation accuracy: 0.283.. \n",
            "Epoch 18/50..  Training loss: 2.527..  Validation loss: 2.508..  Validation accuracy: 0.298.. \n",
            "Epoch 19/50..  Training loss: 2.451..  Validation loss: 2.487..  Validation accuracy: 0.339.. \n",
            "Epoch 20/50..  Training loss: 2.415..  Validation loss: 2.481..  Validation accuracy: 0.334.. \n",
            "Epoch 21/50..  Training loss: 2.501..  Validation loss: 2.476..  Validation accuracy: 0.334.. \n",
            "Epoch 22/50..  Training loss: 2.511..  Validation loss: 2.467..  Validation accuracy: 0.350.. \n",
            "Epoch 23/50..  Training loss: 2.481..  Validation loss: 2.463..  Validation accuracy: 0.350.. \n",
            "Epoch 24/50..  Training loss: 2.418..  Validation loss: 2.466..  Validation accuracy: 0.350.. \n",
            "Epoch 25/50..  Training loss: 2.475..  Validation loss: 2.468..  Validation accuracy: 0.350.. \n",
            "Epoch 26/50..  Training loss: 2.441..  Validation loss: 2.478..  Validation accuracy: 0.334.. \n",
            "Epoch 27/50..  Training loss: 2.478..  Validation loss: 2.480..  Validation accuracy: 0.334.. \n",
            "Epoch 28/50..  Training loss: 2.520..  Validation loss: 2.479..  Validation accuracy: 0.334.. \n",
            "Epoch 29/50..  Training loss: 2.396..  Validation loss: 2.479..  Validation accuracy: 0.334.. \n",
            "Epoch 30/50..  Training loss: 2.482..  Validation loss: 2.474..  Validation accuracy: 0.342.. \n",
            "Epoch 31/50..  Training loss: 2.498..  Validation loss: 2.473..  Validation accuracy: 0.342.. \n",
            "Epoch 32/50..  Training loss: 2.486..  Validation loss: 2.472..  Validation accuracy: 0.350.. \n",
            "Epoch 33/50..  Training loss: 2.496..  Validation loss: 2.470..  Validation accuracy: 0.350.. \n",
            "Epoch 34/50..  Training loss: 2.456..  Validation loss: 2.472..  Validation accuracy: 0.342.. \n",
            "Epoch 35/50..  Training loss: 2.466..  Validation loss: 2.470..  Validation accuracy: 0.342.. \n",
            "Epoch 36/50..  Training loss: 2.499..  Validation loss: 2.463..  Validation accuracy: 0.350.. \n",
            "Epoch 37/50..  Training loss: 2.422..  Validation loss: 2.458..  Validation accuracy: 0.350.. \n",
            "Epoch 38/50..  Training loss: 2.485..  Validation loss: 2.458..  Validation accuracy: 0.350.. \n",
            "Epoch 39/50..  Training loss: 2.469..  Validation loss: 2.452..  Validation accuracy: 0.360.. \n",
            "Epoch 40/50..  Training loss: 2.494..  Validation loss: 2.457..  Validation accuracy: 0.360.. \n",
            "Epoch 41/50..  Training loss: 2.473..  Validation loss: 2.457..  Validation accuracy: 0.360.. \n",
            "Epoch 42/50..  Training loss: 2.423..  Validation loss: 2.459..  Validation accuracy: 0.350.. \n",
            "Epoch 43/50..  Training loss: 2.446..  Validation loss: 2.461..  Validation accuracy: 0.350.. \n",
            "Epoch 44/50..  Training loss: 2.437..  Validation loss: 2.461..  Validation accuracy: 0.350.. \n",
            "Epoch 45/50..  Training loss: 2.435..  Validation loss: 2.462..  Validation accuracy: 0.350.. \n",
            "Epoch 46/50..  Training loss: 2.441..  Validation loss: 2.462..  Validation accuracy: 0.350.. \n",
            "Epoch 47/50..  Training loss: 2.495..  Validation loss: 2.462..  Validation accuracy: 0.350.. \n",
            "Epoch 48/50..  Training loss: 2.498..  Validation loss: 2.462..  Validation accuracy: 0.350.. \n",
            "Epoch 49/50..  Training loss: 2.492..  Validation loss: 2.463..  Validation accuracy: 0.350.. \n",
            "Epoch 50/50..  Training loss: 2.486..  Validation loss: 2.463..  Validation accuracy: 0.350.. \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}