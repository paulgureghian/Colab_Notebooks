{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Medetec_Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulgureghian/Google_Colab_PyTorch_Notebooks/blob/master/Medetec_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fqQih7hWMmaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0e71ee13-5a7b-45c2-9958-f81a05cfdece"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lwomz8yNOD5M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from collections import OrderedDict\n",
        "from torch.optim import lr_scheduler \n",
        "from torchvision import  models, datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpJdybPMOgfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/Medetec/data/medetec_dataset'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/valid'\n",
        "test_dir = data_dir + '/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RPI0h2et6UWk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_transforms= transforms.Compose([transforms.RandomRotation(45),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.RandomCrop(224),\n",
        "                                         transforms.RandomResizedCrop(224),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                              (0.229,0.224,0.255))])\n",
        "\n",
        "validation_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                            transforms.CenterCrop(224),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                                 (0.229,0.224,0.255))])\n",
        "\n",
        "testing_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                         transforms.CenterCrop(224),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                              (0.229,0.224,0.255))])\n",
        "\n",
        "training_dataset = datasets.ImageFolder(train_dir, transform=training_transforms)\n",
        "validation_dataset = datasets.ImageFolder(valid_dir, transform=validation_transforms)\n",
        "testing_dataset = datasets.ImageFolder(test_dir, transform=testing_transforms)\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=32)\n",
        "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=32) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "msdlit-I_Qm9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "8659a5c7-3551-4426-ddf0-8d64e0a8a3c8"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = models.vgg19(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad_(False) \n",
        "    \n",
        "model.classifier = nn.Sequential(OrderedDict([\n",
        "                                ('fc1', nn.Linear(25088, 4096)),\n",
        "                                ('relu1', nn.ReLU()),                                                             \n",
        "                                ('fc2', nn.Linear(4096, 15)), \n",
        "                                ('output', nn.Softmax(dim=1))\n",
        "                                ])) \n",
        "\n",
        "epochs = 50\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.0002)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.3) \n",
        "\n",
        "model.to(device) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (fc1): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (relu1): ReLU()\n",
              "    (fc2): Linear(in_features=4096, out_features=15, bias=True)\n",
              "    (output): Softmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "_e2ZWlc7TKTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "62626f06-511d-4565-80b4-1adeea4526e8"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    scheduler.step()\n",
        "    train_loss = 0\n",
        "    \n",
        "    model.train()\n",
        "    for inputs, labels in training_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logps = model.forward(inputs)\n",
        "        \n",
        "        loss = criterion(logps, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    model.eval()\n",
        "    accuracy = 0\n",
        "    valid_loss = 0\n",
        "    \n",
        "    for inputs, labels in validation_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        output = model.forward(inputs)\n",
        "        \n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        valid_loss += loss.item()\n",
        "        \n",
        "        ps = torch.exp(output)\n",
        "        top_ps, top_class = ps.topk(1, dim=1) \n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "        \n",
        "    print('Epoch {}/{}.. '.format(epoch+1, epochs),\n",
        "          'Training loss: {:.3f}.. '.format(train_loss / len(training_dataloader)),\n",
        "          'Validation loss: {:.3f}.. '.format(valid_loss / len(validation_dataloader)),\n",
        "          'Validation accuracy: {:.3f}.. '.format(accuracy / len(validation_dataloader)))\n",
        "          \n",
        "    model.train()      "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50..  Training loss: 2.625..  Validation loss: 2.606..  Validation accuracy: 0.211.. \n",
            "Epoch 2/50..  Training loss: 2.569..  Validation loss: 2.606..  Validation accuracy: 0.211.. \n",
            "Epoch 3/50..  Training loss: 2.569..  Validation loss: 2.606..  Validation accuracy: 0.211.. \n",
            "Epoch 4/50..  Training loss: 2.530..  Validation loss: 2.605..  Validation accuracy: 0.211.. \n",
            "Epoch 5/50..  Training loss: 2.549..  Validation loss: 2.617..  Validation accuracy: 0.191.. \n",
            "Epoch 6/50..  Training loss: 2.559..  Validation loss: 2.558..  Validation accuracy: 0.261.. \n",
            "Epoch 7/50..  Training loss: 2.521..  Validation loss: 2.570..  Validation accuracy: 0.250.. \n",
            "Epoch 8/50..  Training loss: 2.553..  Validation loss: 2.560..  Validation accuracy: 0.253.. \n",
            "Epoch 9/50..  Training loss: 2.494..  Validation loss: 2.549..  Validation accuracy: 0.269.. \n",
            "Epoch 10/50..  Training loss: 2.505..  Validation loss: 2.566..  Validation accuracy: 0.245.. \n",
            "Epoch 11/50..  Training loss: 2.512..  Validation loss: 2.560..  Validation accuracy: 0.253.. \n",
            "Epoch 12/50..  Training loss: 2.519..  Validation loss: 2.532..  Validation accuracy: 0.284.. \n",
            "Epoch 13/50..  Training loss: 2.469..  Validation loss: 2.506..  Validation accuracy: 0.311.. \n",
            "Epoch 14/50..  Training loss: 2.519..  Validation loss: 2.516..  Validation accuracy: 0.300.. \n",
            "Epoch 15/50..  Training loss: 2.513..  Validation loss: 2.519..  Validation accuracy: 0.292.. \n",
            "Epoch 16/50..  Training loss: 2.536..  Validation loss: 2.514..  Validation accuracy: 0.300.. \n",
            "Epoch 17/50..  Training loss: 2.453..  Validation loss: 2.518..  Validation accuracy: 0.300.. \n",
            "Epoch 18/50..  Training loss: 2.528..  Validation loss: 2.512..  Validation accuracy: 0.300.. \n",
            "Epoch 19/50..  Training loss: 2.533..  Validation loss: 2.494..  Validation accuracy: 0.324.. \n",
            "Epoch 20/50..  Training loss: 2.502..  Validation loss: 2.490..  Validation accuracy: 0.329.. \n",
            "Epoch 21/50..  Training loss: 2.545..  Validation loss: 2.486..  Validation accuracy: 0.334.. \n",
            "Epoch 22/50..  Training loss: 2.537..  Validation loss: 2.475..  Validation accuracy: 0.335.. \n",
            "Epoch 23/50..  Training loss: 2.534..  Validation loss: 2.481..  Validation accuracy: 0.335.. \n",
            "Epoch 24/50..  Training loss: 2.484..  Validation loss: 2.436..  Validation accuracy: 0.385.. \n",
            "Epoch 25/50..  Training loss: 2.525..  Validation loss: 2.435..  Validation accuracy: 0.371.. \n",
            "Epoch 26/50..  Training loss: 2.494..  Validation loss: 2.445..  Validation accuracy: 0.368.. \n",
            "Epoch 27/50..  Training loss: 2.516..  Validation loss: 2.464..  Validation accuracy: 0.346.. \n",
            "Epoch 28/50..  Training loss: 2.459..  Validation loss: 2.478..  Validation accuracy: 0.337.. \n",
            "Epoch 29/50..  Training loss: 2.484..  Validation loss: 2.533..  Validation accuracy: 0.268.. \n",
            "Epoch 30/50..  Training loss: 2.511..  Validation loss: 2.457..  Validation accuracy: 0.363.. \n",
            "Epoch 31/50..  Training loss: 2.548..  Validation loss: 2.465..  Validation accuracy: 0.353.. \n",
            "Epoch 32/50..  Training loss: 2.476..  Validation loss: 2.504..  Validation accuracy: 0.304.. \n",
            "Epoch 33/50..  Training loss: 2.437..  Validation loss: 2.476..  Validation accuracy: 0.332.. \n",
            "Epoch 34/50..  Training loss: 2.464..  Validation loss: 2.464..  Validation accuracy: 0.345.. \n",
            "Epoch 35/50..  Training loss: 2.510..  Validation loss: 2.462..  Validation accuracy: 0.346.. \n",
            "Epoch 36/50..  Training loss: 2.461..  Validation loss: 2.460..  Validation accuracy: 0.357.. \n",
            "Epoch 37/50..  Training loss: 2.510..  Validation loss: 2.471..  Validation accuracy: 0.343.. \n",
            "Epoch 38/50..  Training loss: 2.461..  Validation loss: 2.477..  Validation accuracy: 0.336.. \n",
            "Epoch 39/50..  Training loss: 2.482..  Validation loss: 2.473..  Validation accuracy: 0.343.. \n",
            "Epoch 40/50..  Training loss: 2.450..  Validation loss: 2.472..  Validation accuracy: 0.343.. \n",
            "Epoch 41/50..  Training loss: 2.522..  Validation loss: 2.476..  Validation accuracy: 0.336.. \n",
            "Epoch 42/50..  Training loss: 2.469..  Validation loss: 2.480..  Validation accuracy: 0.336.. \n",
            "Epoch 43/50..  Training loss: 2.474..  Validation loss: 2.469..  Validation accuracy: 0.359.. \n",
            "Epoch 44/50..  Training loss: 2.405..  Validation loss: 2.467..  Validation accuracy: 0.351.. \n",
            "Epoch 45/50..  Training loss: 2.457..  Validation loss: 2.467..  Validation accuracy: 0.351.. \n",
            "Epoch 46/50..  Training loss: 2.446..  Validation loss: 2.476..  Validation accuracy: 0.336.. \n",
            "Epoch 47/50..  Training loss: 2.404..  Validation loss: 2.474..  Validation accuracy: 0.339.. \n",
            "Epoch 48/50..  Training loss: 2.484..  Validation loss: 2.475..  Validation accuracy: 0.336.. \n",
            "Epoch 49/50..  Training loss: 2.480..  Validation loss: 2.469..  Validation accuracy: 0.343.. \n",
            "Epoch 50/50..  Training loss: 2.477..  Validation loss: 2.463..  Validation accuracy: 0.354.. \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}