{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Medetec_Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulgureghian/Google_Colab_PyTorch_Notebooks/blob/master/Medetec_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fqQih7hWMmaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0ad04ee4-9098-4f86-a1dd-6cc652ef7c9a"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lwomz8yNOD5M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from collections import OrderedDict\n",
        "from torch.optim import lr_scheduler \n",
        "from torchvision import  models, datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpJdybPMOgfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/Medetec/data/medetec_dataset'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/valid'\n",
        "test_dir = data_dir + '/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RPI0h2et6UWk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_transforms= transforms.Compose([transforms.RandomRotation(45),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.RandomCrop(224),\n",
        "                                         transforms.RandomResizedCrop(224),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                              (0.229,0.224,0.255))])\n",
        "\n",
        "validation_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                            transforms.CenterCrop(224),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                                 (0.229,0.224,0.255))])\n",
        "\n",
        "testing_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                         transforms.CenterCrop(224),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                              (0.229,0.224,0.255))])\n",
        "\n",
        "training_dataset = datasets.ImageFolder(train_dir, transform=training_transforms)\n",
        "validation_dataset = datasets.ImageFolder(valid_dir, transform=validation_transforms)\n",
        "testing_dataset = datasets.ImageFolder(test_dir, transform=testing_transforms)\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=32)\n",
        "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=32) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "msdlit-I_Qm9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = models.vgg19(pretrained=True)\n",
        "\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad_(False) \n",
        "    \n",
        "model.classifier = nn.Sequential(OrderedDict([\n",
        "                                ('fc1', nn.Linear(25088, 4096)),\n",
        "                                ('relu1', nn.ReLU()),                                                             \n",
        "                                ('fc2', nn.Linear(4096, 15)), \n",
        "                                ('output', nn.Softmax(dim=1))\n",
        "                                ])) \n",
        "\n",
        "model.to(device) \n",
        "epochs = 50\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.0001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_e2ZWlc7TKTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "fba8c18d-395b-4f84-db5d-bcfe4ce8cdc9"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    scheduler.step()\n",
        "    train_loss = 0\n",
        "    \n",
        "    model.train()\n",
        "    for inputs, labels in training_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logps = model.forward(inputs)\n",
        "        \n",
        "        loss = criterion(logps, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    model.eval()\n",
        "    accuracy = 0\n",
        "    valid_loss = 0\n",
        "    \n",
        "    for inputs, labels in validation_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        output = model.forward(inputs)\n",
        "        \n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        valid_loss += loss.item()\n",
        "        \n",
        "        ps = torch.exp(output)\n",
        "        top_ps, top_class = ps.topk(1, dim=1) \n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "        \n",
        "    print('Epoch {}/{}.. '.format(epoch+1, epochs),\n",
        "          'Training loss: {:.3f}.. '.format(train_loss / len(training_dataloader)),\n",
        "          'Validation loss: {:.3f}.. '.format(valid_loss / len(validation_dataloader)),\n",
        "          'Validation accuracy: {:.3f}.. '.format(accuracy / len(validation_dataloader)))\n",
        "          \n",
        "    model.train()      "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50..  Training loss: 2.568..  Validation loss: 2.520..  Validation accuracy: 0.300.. \n",
            "Epoch 2/50..  Training loss: 2.516..  Validation loss: 2.495..  Validation accuracy: 0.323.. \n",
            "Epoch 3/50..  Training loss: 2.494..  Validation loss: 2.489..  Validation accuracy: 0.316.. \n",
            "Epoch 4/50..  Training loss: 2.495..  Validation loss: 2.512..  Validation accuracy: 0.300.. \n",
            "Epoch 5/50..  Training loss: 2.488..  Validation loss: 2.516..  Validation accuracy: 0.300.. \n",
            "Epoch 6/50..  Training loss: 2.524..  Validation loss: 2.524..  Validation accuracy: 0.292.. \n",
            "Epoch 7/50..  Training loss: 2.537..  Validation loss: 2.537..  Validation accuracy: 0.269.. \n",
            "Epoch 8/50..  Training loss: 2.493..  Validation loss: 2.507..  Validation accuracy: 0.308.. \n",
            "Epoch 9/50..  Training loss: 2.504..  Validation loss: 2.535..  Validation accuracy: 0.276.. \n",
            "Epoch 10/50..  Training loss: 2.482..  Validation loss: 2.511..  Validation accuracy: 0.316.. \n",
            "Epoch 11/50..  Training loss: 2.509..  Validation loss: 2.535..  Validation accuracy: 0.284.. \n",
            "Epoch 12/50..  Training loss: 2.517..  Validation loss: 2.520..  Validation accuracy: 0.300.. \n",
            "Epoch 13/50..  Training loss: 2.494..  Validation loss: 2.515..  Validation accuracy: 0.300.. \n",
            "Epoch 14/50..  Training loss: 2.470..  Validation loss: 2.509..  Validation accuracy: 0.308.. \n",
            "Epoch 15/50..  Training loss: 2.476..  Validation loss: 2.504..  Validation accuracy: 0.303.. \n",
            "Epoch 16/50..  Training loss: 2.509..  Validation loss: 2.505..  Validation accuracy: 0.314.. \n",
            "Epoch 17/50..  Training loss: 2.466..  Validation loss: 2.501..  Validation accuracy: 0.320.. \n",
            "Epoch 18/50..  Training loss: 2.467..  Validation loss: 2.492..  Validation accuracy: 0.339.. \n",
            "Epoch 19/50..  Training loss: 2.460..  Validation loss: 2.508..  Validation accuracy: 0.290.. \n",
            "Epoch 20/50..  Training loss: 2.455..  Validation loss: 2.510..  Validation accuracy: 0.314.. \n",
            "Epoch 21/50..  Training loss: 2.463..  Validation loss: 2.509..  Validation accuracy: 0.298.. \n",
            "Epoch 22/50..  Training loss: 2.500..  Validation loss: 2.512..  Validation accuracy: 0.290.. \n",
            "Epoch 23/50..  Training loss: 2.507..  Validation loss: 2.514..  Validation accuracy: 0.298.. \n",
            "Epoch 24/50..  Training loss: 2.417..  Validation loss: 2.514..  Validation accuracy: 0.306.. \n",
            "Epoch 25/50..  Training loss: 2.409..  Validation loss: 2.514..  Validation accuracy: 0.290.. \n",
            "Epoch 26/50..  Training loss: 2.514..  Validation loss: 2.505..  Validation accuracy: 0.298.. \n",
            "Epoch 27/50..  Training loss: 2.487..  Validation loss: 2.501..  Validation accuracy: 0.306.. \n",
            "Epoch 28/50..  Training loss: 2.467..  Validation loss: 2.504..  Validation accuracy: 0.306.. \n",
            "Epoch 29/50..  Training loss: 2.469..  Validation loss: 2.505..  Validation accuracy: 0.306.. \n",
            "Epoch 30/50..  Training loss: 2.411..  Validation loss: 2.506..  Validation accuracy: 0.298.. \n",
            "Epoch 31/50..  Training loss: 2.485..  Validation loss: 2.510..  Validation accuracy: 0.298.. \n",
            "Epoch 32/50..  Training loss: 2.450..  Validation loss: 2.509..  Validation accuracy: 0.306.. \n",
            "Epoch 33/50..  Training loss: 2.425..  Validation loss: 2.505..  Validation accuracy: 0.306.. \n",
            "Epoch 34/50..  Training loss: 2.408..  Validation loss: 2.500..  Validation accuracy: 0.325.. \n",
            "Epoch 35/50..  Training loss: 2.486..  Validation loss: 2.501..  Validation accuracy: 0.309.. \n",
            "Epoch 36/50..  Training loss: 2.451..  Validation loss: 2.510..  Validation accuracy: 0.317.. \n",
            "Epoch 37/50..  Training loss: 2.481..  Validation loss: 2.511..  Validation accuracy: 0.317.. \n",
            "Epoch 38/50..  Training loss: 2.394..  Validation loss: 2.514..  Validation accuracy: 0.317.. \n",
            "Epoch 39/50..  Training loss: 2.491..  Validation loss: 2.515..  Validation accuracy: 0.298.. \n",
            "Epoch 40/50..  Training loss: 2.428..  Validation loss: 2.527..  Validation accuracy: 0.278.. \n",
            "Epoch 41/50..  Training loss: 2.444..  Validation loss: 2.528..  Validation accuracy: 0.278.. \n",
            "Epoch 42/50..  Training loss: 2.440..  Validation loss: 2.528..  Validation accuracy: 0.278.. \n",
            "Epoch 43/50..  Training loss: 2.431..  Validation loss: 2.527..  Validation accuracy: 0.278.. \n",
            "Epoch 44/50..  Training loss: 2.470..  Validation loss: 2.526..  Validation accuracy: 0.278.. \n",
            "Epoch 45/50..  Training loss: 2.400..  Validation loss: 2.524..  Validation accuracy: 0.278.. \n",
            "Epoch 46/50..  Training loss: 2.458..  Validation loss: 2.523..  Validation accuracy: 0.278.. \n",
            "Epoch 47/50..  Training loss: 2.423..  Validation loss: 2.522..  Validation accuracy: 0.275.. \n",
            "Epoch 48/50..  Training loss: 2.465..  Validation loss: 2.523..  Validation accuracy: 0.275.. \n",
            "Epoch 49/50..  Training loss: 2.425..  Validation loss: 2.524..  Validation accuracy: 0.275.. \n",
            "Epoch 50/50..  Training loss: 2.478..  Validation loss: 2.524..  Validation accuracy: 0.267.. \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}